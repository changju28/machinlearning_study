boosting(~ 전보다 더 크거나 높게 하다)
    1. 일부러 성능이 않좋은 모델들을 사용한다
    2. 더 먼저 만든 모델들의 성능이, 뒤에 있는 모델이 사용할 데이터셋을 바꾼다.
    3. 모델들의 예측을 종합할 때, 성능이 좋은 모델의 예측을 더 반영한다.

        핵심: 선능이 안 좋은 약한 학습자(weak learner) 들을 합쳐서 성능을 극대화한다.

    에다 부스트 (adaboost)
        1) 성능이 좋지 않은 결정 스텀프를 만이 만든다.(weak learners)
        2) 각 스텀프는 전에 왔던 스텀프들이 틀린 데이터들을 더 중요하게 맞춘다.
        3) 예측을 종합할 때 성능이 좋은 스텀프의 의견 비중을 더 높게 반영한다.
            스텀프의 성능 공식 : 1/2log(1 - total_error / total_error)


    스텀프 생성과 성능
        1) 스텀프는 결정 트리를 만들 때랑 똑같이 지니 불순도를 이용해서 만듭니다.
        2) 모든 데이터에는 중요도라는 게 있습니다. 이 중요도는 에다 부스트 알고리즘에서 계속 사용할 건데요. 일단 이번 레슨에서는 스텀프의 오류, total_error를 계산할 때 사용햇습니다.
            total error는 틀리게 예측한 모든 데이터 중요도의 합니다.
        3) 이 total_error는 스텀프의 성능을 계산하는 데 사용됩니다.